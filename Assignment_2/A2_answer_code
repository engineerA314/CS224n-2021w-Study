1. 시그모이드 함수

    s = 1 / (1 + numpy.exp(-x))
    
    
   
2. 나이브 소프트맥스 로스/기울기

    input_to_softmax = np.dot(outsideVectors, centerWordVec)
    naive_softmax = softmax(input_to_softmax)
    loss = - numpy.log(naive_softmax[outsideWordIdx])
    naive_minus_y = naive_softmax.copy()
    naive_minus_y[outsideWordIdx] -= 1
    gradCenterVec = np.dot(naive_minus_y, outsideVectors)
    #gradOutsideVecs = np.dot(naive_minus_y, centerWordVec.T) 불가능하다. 1D matrix는 .T 가 작동하지 않는다
    #따라서 dot 연산 대신에 outer 연산을 사용해야 한다
    gradOutsideVecs = np.outer(naive_minus_y, centerWordVec)
    
    
    
3. 네거티브샘플


    sigmoid_outside = sigmoid(np.dot(outsideVectors[outsideWordIdx], centerWordVec))
    samples = outsideVectors[negSampleWordIndices]
    sigmoid_sample = sigmoid(-np.dot(samples, centerWordVec))

    loss = -np.log(sigmoid_outside) - np.sum(np.log(sigmoid_sample))

    gradCenterVec = np.dot(outsideVectors[outsideWordIdx].T, sigmoid_outside - 1) - np.dot(samples.T, sigmoid_sample - 1)

    gradOutsideVecs = np.zeros(outsideVectors.shape)
    gradOutsideVecs[outsideWordIdx] += np.dot(sigmoid_outside - 1, centerWordVec)
    sample_grad = np.outer(1 - sigmoid_sample, centerWordVec)
    for i, sample in enumerate(negSampleWordIndices):
        gradOutsideVecs[sample] += sample_grad[i]
        
        
4. Skip-gram


    centerWordIdx = word2Ind[currentCenterWord]
    for word in outsideWords:
        index = word2Ind[word]
        WordLoss, centerGrad, outGrad = word2vecLossAndGradient(centerWordVectors[centerWordIdx], index, outsideVectors, dataset)
        loss += WordLoss
        gradCenterVecs[centerWordIdx] += centerGrad
        gradOutsideVectors += outGrad


